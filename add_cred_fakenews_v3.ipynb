{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "add_cred_fakenews_v3.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "jhIWDFaBoEY0",
        "6Mb30c58Byq0",
        "dj_wNk4ibwqm",
        "DSZ81ypY8Lud",
        "7uqrNtwUz4so",
        "FMxhC2BqF4Nu",
        "fSzdqzsD46HY",
        "eCEeJbhoXSXP",
        "xCWdm8IRXUlW"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4d1f54d4e8ac4aa98a4436afab356362": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_ed6b60230a9b4389b3f754d8a0228a9c",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_a109cf24ae4b48efa3ebac7408c5a9a4",
              "IPY_MODEL_52fac2971efb420d8af410a1d4628a60"
            ]
          }
        },
        "ed6b60230a9b4389b3f754d8a0228a9c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a109cf24ae4b48efa3ebac7408c5a9a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_d606b3dbf212480b8da17bf6da0efbd0",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 434,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 434,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8f26f29a194d47d48d9020673eca3aa4"
          }
        },
        "52fac2971efb420d8af410a1d4628a60": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_1a988571cbb440b4a9434280945bce59",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 434/434 [00:00&lt;00:00, 1.56kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_fe002531c6ef4a8f9bc4754fb3742e40"
          }
        },
        "d606b3dbf212480b8da17bf6da0efbd0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8f26f29a194d47d48d9020673eca3aa4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1a988571cbb440b4a9434280945bce59": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "fe002531c6ef4a8f9bc4754fb3742e40": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "013053b03cb44a21a97c342d734e38e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_47e138e2e8404964a328ff6dc241a747",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_87a3b8eb5a51498e8576fc7038d5179f",
              "IPY_MODEL_c94c32718be547bfba8a248ffc1ec798"
            ]
          }
        },
        "47e138e2e8404964a328ff6dc241a747": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "87a3b8eb5a51498e8576fc7038d5179f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_521af8d25735462cbe341271bc41b31c",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1215509,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1215509,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a523b8574ed44450bfc202635dcd92ce"
          }
        },
        "c94c32718be547bfba8a248ffc1ec798": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_a40c644c422b428095bed5c2649f59b4",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.22M/1.22M [00:00&lt;00:00, 11.0MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d37c08f870824c7b9a7382356680a62c"
          }
        },
        "521af8d25735462cbe341271bc41b31c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a523b8574ed44450bfc202635dcd92ce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a40c644c422b428095bed5c2649f59b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d37c08f870824c7b9a7382356680a62c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c0e22827cfc24d81ad6aec92088812a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_af068e9b4ab146258a582f1acdd67d74",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_acab944c30a34150867140b62e320215",
              "IPY_MODEL_3ffab281780042038223b8c73f52d96c"
            ]
          }
        },
        "af068e9b4ab146258a582f1acdd67d74": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "acab944c30a34150867140b62e320215": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_f8de5d0bda844d04b154e95e164e3082",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 963211760,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 963211760,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9bea7ef423414a36b22aa7d5aedc59d8"
          }
        },
        "3ffab281780042038223b8c73f52d96c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_abe4a796bb484d57a50dd7f5eb88f63a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 963M/963M [00:28&lt;00:00, 33.8MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5419fbd1f4bf4947b5e92a21889b81ac"
          }
        },
        "f8de5d0bda844d04b154e95e164e3082": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9bea7ef423414a36b22aa7d5aedc59d8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "abe4a796bb484d57a50dd7f5eb88f63a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5419fbd1f4bf4947b5e92a21889b81ac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mahsaghn/FakeNewsDetection/blob/baseline/add_cred_fakenews_v3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhIWDFaBoEY0"
      },
      "source": [
        "#Install"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-OGQzlboBKI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d60db56f-28fe-49ac-c990-0ba6a905f82e"
      },
      "source": [
        "!pip install -U nltk\n",
        "!pip install pandas\n",
        "!pip install hazm\n",
        "!pip install sklearn\n",
        "!pip install numpy\n",
        "!pip install flair\n",
        "!pip install stanfordnlp\n",
        "!pip install torch"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Collecting nltk\n",
            "  Downloading nltk-3.6.2-py3-none-any.whl (1.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5 MB 30.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.41.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from nltk) (2019.12.20)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n",
            "Installing collected packages: nltk\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "Successfully installed nltk-3.6.2\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.1.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.1)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.19.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
            "Collecting hazm\n",
            "  Downloading hazm-0.7.0-py3-none-any.whl (316 kB)\n",
            "\u001b[K     |████████████████████████████████| 316 kB 31.5 MB/s \n",
            "\u001b[?25hCollecting libwapiti>=0.2.1\n",
            "  Downloading libwapiti-0.2.1.tar.gz (233 kB)\n",
            "\u001b[K     |████████████████████████████████| 233 kB 61.5 MB/s \n",
            "\u001b[?25hCollecting nltk==3.3\n",
            "  Downloading nltk-3.3.0.zip (1.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4 MB 52.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk==3.3->hazm) (1.15.0)\n",
            "Building wheels for collected packages: nltk, libwapiti\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.3-py3-none-any.whl size=1394488 sha256=d2d22e715e1593eafb06ad46842570d64c8d0a0076ed954bf788d5ec8e34cd60\n",
            "  Stored in directory: /root/.cache/pip/wheels/9b/fd/0c/d92302c876e5de87ebd7fc0979d82edb93e2d8d768bf71fac4\n",
            "  Building wheel for libwapiti (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for libwapiti: filename=libwapiti-0.2.1-cp37-cp37m-linux_x86_64.whl size=154060 sha256=8c3dbeb7f562dc29ae46357af43c5fc5a55de569081cdaa9972c48cf9d9bf71d\n",
            "  Stored in directory: /root/.cache/pip/wheels/ab/b2/5b/0fe4b8f5c0e65341e8ea7bb3f4a6ebabfe8b1ac31322392dbf\n",
            "Successfully built nltk libwapiti\n",
            "Installing collected packages: nltk, libwapiti, hazm\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.6.2\n",
            "    Uninstalling nltk-3.6.2:\n",
            "      Successfully uninstalled nltk-3.6.2\n",
            "Successfully installed hazm-0.7.0 libwapiti-0.2.1 nltk-3.3\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (0.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn) (0.22.2.post1)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.19.5)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.0.1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.19.5)\n",
            "Collecting flair\n",
            "  Downloading flair-0.8.0.post1-py3-none-any.whl (284 kB)\n",
            "\u001b[K     |████████████████████████████████| 284 kB 28.3 MB/s \n",
            "\u001b[?25hCollecting mpld3==0.3\n",
            "  Downloading mpld3-0.3.tar.gz (788 kB)\n",
            "\u001b[K     |████████████████████████████████| 788 kB 49.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<1.20.0 in /usr/local/lib/python3.7/dist-packages (from flair) (1.19.5)\n",
            "Collecting transformers>=4.0.0\n",
            "  Downloading transformers-4.8.2-py3-none-any.whl (2.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.5 MB 55.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: gensim<=3.8.3,>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from flair) (3.6.0)\n",
            "Collecting ftfy\n",
            "  Downloading ftfy-6.0.3.tar.gz (64 kB)\n",
            "\u001b[K     |████████████████████████████████| 64 kB 3.4 MB/s \n",
            "\u001b[?25hCollecting janome\n",
            "  Downloading Janome-0.4.1-py2.py3-none-any.whl (19.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 19.7 MB 35.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from flair) (4.2.6)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from flair) (2.8.1)\n",
            "Collecting sentencepiece==0.1.95\n",
            "  Downloading sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 52.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from flair) (0.22.2.post1)\n",
            "Collecting bpemb>=0.3.2\n",
            "  Downloading bpemb-0.3.3-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from flair) (2019.12.20)\n",
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[K     |████████████████████████████████| 981 kB 47.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from flair) (0.8.9)\n",
            "Collecting sqlitedict>=1.6.0\n",
            "  Downloading sqlitedict-1.7.0.tar.gz (28 kB)\n",
            "Collecting torch<=1.7.1,>=1.5.0\n",
            "  Downloading torch-1.7.1-cp37-cp37m-manylinux1_x86_64.whl (776.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 776.8 MB 17 kB/s \n",
            "\u001b[?25hRequirement already satisfied: hyperopt>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from flair) (0.1.2)\n",
            "Requirement already satisfied: tqdm>=4.26.0 in /usr/local/lib/python3.7/dist-packages (from flair) (4.41.1)\n",
            "Collecting segtok>=1.5.7\n",
            "  Downloading segtok-1.5.10.tar.gz (25 kB)\n",
            "Requirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.7/dist-packages (from flair) (3.2.2)\n",
            "Collecting deprecated>=1.2.4\n",
            "  Downloading Deprecated-1.2.12-py2.py3-none-any.whl (9.5 kB)\n",
            "Collecting gdown==3.12.2\n",
            "  Downloading gdown-3.12.2.tar.gz (8.2 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting huggingface-hub\n",
            "  Downloading huggingface_hub-0.0.14-py3-none-any.whl (43 kB)\n",
            "\u001b[K     |████████████████████████████████| 43 kB 1.7 MB/s \n",
            "\u001b[?25hCollecting konoha<5.0.0,>=4.0.0\n",
            "  Downloading konoha-4.6.5-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from gdown==3.12.2->flair) (3.0.12)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gdown==3.12.2->flair) (1.15.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.7/dist-packages (from gdown==3.12.2->flair) (2.23.0)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.7/dist-packages (from deprecated>=1.2.4->flair) (1.12.1)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim<=3.8.3,>=3.4.0->flair) (1.4.1)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim<=3.8.3,>=3.4.0->flair) (5.1.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from hyperopt>=0.1.1->flair) (2.5.1)\n",
            "Requirement already satisfied: pymongo in /usr/local/lib/python3.7/dist-packages (from hyperopt>=0.1.1->flair) (3.11.4)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from hyperopt>=0.1.1->flair) (0.16.0)\n",
            "Collecting requests\n",
            "  Downloading requests-2.26.0-py2.py3-none-any.whl (62 kB)\n",
            "\u001b[K     |████████████████████████████████| 62 kB 974 kB/s \n",
            "\u001b[?25hCollecting importlib-metadata<4.0.0,>=3.7.0\n",
            "  Downloading importlib_metadata-3.10.1-py3-none-any.whl (14 kB)\n",
            "Collecting overrides<4.0.0,>=3.0.0\n",
            "  Downloading overrides-3.1.0.tar.gz (11 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata<4.0.0,>=3.7.0->konoha<5.0.0,>=4.0.0->flair) (3.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata<4.0.0,>=3.7.0->konoha<5.0.0,>=4.0.0->flair) (3.7.4.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->flair) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->flair) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->flair) (2.4.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb>=0.3.2->flair) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb>=0.3.2->flair) (2021.5.30)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb>=0.3.2->flair) (2.0.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb>=0.3.2->flair) (2.10)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->flair) (1.0.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers>=4.0.0->flair) (21.0)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 54.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from transformers>=4.0.0->flair) (3.13)\n",
            "Collecting huggingface-hub\n",
            "  Downloading huggingface_hub-0.0.12-py3-none-any.whl (37 kB)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 47.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from ftfy->flair) (0.2.5)\n",
            "Requirement already satisfied: decorator<5,>=4.3 in /usr/local/lib/python3.7/dist-packages (from networkx->hyperopt>=0.1.1->flair) (4.4.2)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb>=0.3.2->flair) (1.7.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=4.0.0->flair) (7.1.2)\n",
            "Building wheels for collected packages: gdown, mpld3, overrides, segtok, sqlitedict, ftfy, langdetect\n",
            "  Building wheel for gdown (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gdown: filename=gdown-3.12.2-py3-none-any.whl size=9705 sha256=433cbc2f5514dc2fd22caae9dc4d4555884c2d2894a164e3db43b7354786d6c3\n",
            "  Stored in directory: /root/.cache/pip/wheels/ba/e0/7e/726e872a53f7358b4b96a9975b04e98113b005cd8609a63abc\n",
            "  Building wheel for mpld3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mpld3: filename=mpld3-0.3-py3-none-any.whl size=116701 sha256=73c45057d4e35d7c8dd896b491dad1d706aae79de15e5269d11d76c72881199c\n",
            "  Stored in directory: /root/.cache/pip/wheels/26/70/6a/1c79e59951a41b4045497da187b2724f5659ca64033cf4548e\n",
            "  Building wheel for overrides (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for overrides: filename=overrides-3.1.0-py3-none-any.whl size=10188 sha256=1994d7963a2f210ad1f67eb81822949c40e4552b7eee9bd16188dc214fba767d\n",
            "  Stored in directory: /root/.cache/pip/wheels/3a/0d/38/01a9bc6e20dcfaf0a6a7b552d03137558ba1c38aea47644682\n",
            "  Building wheel for segtok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for segtok: filename=segtok-1.5.10-py3-none-any.whl size=25030 sha256=2af8ad9320f71b565a73fd3e98946242d15ef9714bf38144a63841e29a876bf4\n",
            "  Stored in directory: /root/.cache/pip/wheels/67/b7/d0/a121106e61339eee5ed083bc230b1c8dc422c49a5a28c2addd\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sqlitedict: filename=sqlitedict-1.7.0-py3-none-any.whl size=14392 sha256=9c71fb5066a98f34b7c291eb26a7c1c585751e7abe2df69c3908a5e1df87b3d9\n",
            "  Stored in directory: /root/.cache/pip/wheels/af/94/06/18c0e83e9e227da8f3582810b51f319bbfd181e508676a56c8\n",
            "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ftfy: filename=ftfy-6.0.3-py3-none-any.whl size=41934 sha256=b4afce50722628c756c8a978f1518b3da522177982ade7973cf750613d17b17c\n",
            "  Stored in directory: /root/.cache/pip/wheels/19/f5/38/273eb3b5e76dfd850619312f693716ac4518b498f5ffb6f56d\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993241 sha256=febc1b92c7fc6e8f7a3654f0c22b9460cb1e87015669b69e7f1246de00a2b968\n",
            "  Stored in directory: /root/.cache/pip/wheels/c5/96/8a/f90c59ed25d75e50a8c10a1b1c2d4c402e4dacfa87f3aff36a\n",
            "Successfully built gdown mpld3 overrides segtok sqlitedict ftfy langdetect\n",
            "Installing collected packages: requests, importlib-metadata, tokenizers, sentencepiece, sacremoses, overrides, huggingface-hub, transformers, torch, sqlitedict, segtok, mpld3, langdetect, konoha, janome, gdown, ftfy, deprecated, bpemb, flair\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib-metadata 4.6.1\n",
            "    Uninstalling importlib-metadata-4.6.1:\n",
            "      Successfully uninstalled importlib-metadata-4.6.1\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.9.0+cu102\n",
            "    Uninstalling torch-1.9.0+cu102:\n",
            "      Successfully uninstalled torch-1.9.0+cu102\n",
            "  Attempting uninstall: gdown\n",
            "    Found existing installation: gdown 3.6.4\n",
            "    Uninstalling gdown-3.6.4:\n",
            "      Successfully uninstalled gdown-3.6.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.10.0+cu102 requires torch==1.9.0, but you have torch 1.7.1 which is incompatible.\n",
            "torchtext 0.10.0 requires torch==1.9.0, but you have torch 1.7.1 which is incompatible.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.26.0 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed bpemb-0.3.3 deprecated-1.2.12 flair-0.8.0.post1 ftfy-6.0.3 gdown-3.12.2 huggingface-hub-0.0.12 importlib-metadata-3.10.1 janome-0.4.1 konoha-4.6.5 langdetect-1.0.9 mpld3-0.3 overrides-3.1.0 requests-2.26.0 sacremoses-0.0.45 segtok-1.5.10 sentencepiece-0.1.95 sqlitedict-1.7.0 tokenizers-0.10.3 torch-1.7.1 transformers-4.8.2\n",
            "Collecting stanfordnlp\n",
            "  Downloading stanfordnlp-0.2.0-py3-none-any.whl (158 kB)\n",
            "\u001b[K     |████████████████████████████████| 158 kB 37.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from stanfordnlp) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from stanfordnlp) (2.26.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from stanfordnlp) (3.17.3)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from stanfordnlp) (1.7.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from stanfordnlp) (4.41.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.0.0->stanfordnlp) (3.7.4.3)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf->stanfordnlp) (1.15.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->stanfordnlp) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->stanfordnlp) (2021.5.30)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests->stanfordnlp) (2.0.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->stanfordnlp) (2.10)\n",
            "Installing collected packages: stanfordnlp\n",
            "Successfully installed stanfordnlp-0.2.0\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.7.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch) (1.19.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Mb30c58Byq0"
      },
      "source": [
        "# Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLQ0-YGbBoFm"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn import metrics\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import stanfordnlp\n",
        "import numpy as np\n",
        "import dill\n",
        "from urllib.parse import urlparse\n",
        "\n",
        "import os.path\n",
        "import joblib\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "from transformers import AutoConfig, AutoTokenizer, TFAutoModel\n",
        "\n",
        "import os\n",
        "import string\n",
        "import pandas as pd\n",
        "from hazm import *\n",
        "import re\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from google.colab import drive\n",
        "import matplotlib.pyplot as plot\n",
        "from keras.utils.vis_utils import plot_model\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from keras import backend as K\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "import dill\n",
        "import math\n",
        "\n",
        "\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from keras.regularizers import l1, l2\n",
        "from keras import regularizers\n",
        "from keras import metrics\n",
        "from keras import backend as K\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dj_wNk4ibwqm"
      },
      "source": [
        "#Google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDqlb6kRb6aO"
      },
      "source": [
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSZ81ypY8Lud"
      },
      "source": [
        "#PS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hPlqj4sb6jDV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b039c26e-dbd2-46a9-94ff-c2e4fd0faecb"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"PSFeatureExtractor.ipynb\n",
        "\n",
        "Automatically generated by Colaboratory.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1-zwppOfr0Cr1k15U5cOBA8Kd3xdKSXWr\n",
        "\"\"\"\n",
        "\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"BaseLineWithGridSearch.ipynb\n",
        "\n",
        "Automatically generated by Colaboratory.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1cxXw92aQZKvWJGmOdKIYGGXRMpK_XNvQ\n",
        "\n",
        "# Import Required Libraries\n",
        "\"\"\"\n",
        "import nltk\n",
        "from nltk import word_tokenize as nltk_word_tokenize\n",
        "import string\n",
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from google.colab import drive\n",
        "import torch\n",
        "import stanfordnlp\n",
        "import itertools\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from hazm import *\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from difflib import SequenceMatcher\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from openpyxl import load_workbook\n",
        "import joblib\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import os.path\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "refute_hedge_reporte_words = ['جعلی',\n",
        "                   'تقلب',\n",
        "                   'فریب',\n",
        "                   'حیله',\n",
        "                   'کلاهبرداری',\n",
        "                   'شیادی',\n",
        "                   'دست انداختن',\n",
        "                   'گول زدن',\n",
        "                   'نادرست',\n",
        "                   'غلط',\n",
        "                   'کذب',\n",
        "                   'ساختگی',\n",
        "                   'قلابی',\n",
        "                   'انکار',\n",
        "                   'رد',\n",
        "                   'تکذیب',\n",
        "                   'تکذیب کردن',\n",
        "                   'تکذیب شد',\n",
        "                   'انکار کردن'\n",
        "                   'انکار می کند',\n",
        "                   'نه',\n",
        "                   'با وجود',\n",
        "                   'علیرغم',\n",
        "                   'با اینکه',\n",
        "                   'شک داشتن',\n",
        "                   'تردید کردن',\n",
        "                   'مظنون بودن',\n",
        "                   'شک',\n",
        "                   'تردید',\n",
        "                   'دو دلی',\n",
        "                   'گمان',\n",
        "                   'به گزارش'\n",
        "                   ,'ادعا شده'\n",
        "                   ,'به قول معروف'\n",
        "                   ,'بنا به گفته'\n",
        "                   , 'ظاهرا'\n",
        "                   ,'به نظر می رسد'\n",
        "                   ,'ادعا'\n",
        "                   ,'میتوانست'\n",
        "                   ,'می تواند'\n",
        "                   ,'از قرار معلوم'\n",
        "                   ,'مشخصا'\n",
        "                   ,'تا حد زیادی'\n",
        "                   ,'احتمال دارد'\n",
        "                   ,'شاید'\n",
        "                   ,'به طور عمده'\n",
        "                   ,'ممکن است'\n",
        "                   ,'گویا'\n",
        "                   ,'ممکن'\n",
        "                   ,'اغلب'\n",
        "                   ,'غالبا'\n",
        "                   ,'احتمالا'\n",
        "                   ,'احتمالاً'\n",
        "                   ,'محتملا'\n",
        "                   ,'گفته شده'\n",
        "                   ,'گزارش داد'\n",
        "                   ,'طبق گزارش'\n",
        "                   ,'شایعه'\n",
        "                   ,'شایعات'\n",
        "                   ,'شایعه شده'\n",
        "                   ,'قدری'\n",
        "                   ,'تا حدی'\n",
        "                   ,'تأیید نشده'\n",
        "]\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "   # stuff only to run when not called via 'import' here\n",
        "   pass\n",
        "\n",
        "\"\"\"# Define Class\"\"\"\n",
        "\n",
        "# -------------------------------------\n",
        "class PSFeatureExtractor():\n",
        "  # -------------------------------------\n",
        "  def __init__(self, dataset_path, stopWord_path, polarity_dataset_path, stanford_models_path , use_google_drive = True, important_words = None\n",
        "  \t,claim_name=\"claim\",headline_name=\"headline\",label_name=\"label\",question_name=\"question\",part_name=\"part\",load_data = True, data = None\n",
        "  \t,uniq_claims={}):\n",
        "    self.dataset_path = dataset_path\n",
        "    self.stopWord_path = stopWord_path\n",
        "    self.polarity_dataset_path = polarity_dataset_path\n",
        "    self.use_google_drive = use_google_drive\n",
        "    self.stanford_models_path = stanford_models_path\n",
        "    self.important_words = important_words\n",
        "    self.clean_claims_headlines = []\n",
        "    self.clean_claims = []\n",
        "    self.clean_headlines = []\n",
        "    self.claim_name= claim_name\n",
        "    self.headline_name = headline_name\n",
        "    self.question_name = question_name\n",
        "    self.label_name = label_name\n",
        "    self.part_name = part_name\n",
        "    self.uniq_claims = uniq_claims\n",
        "\n",
        "    self.fa_stop_words = self.__get_stop_words()\n",
        "    print(\"before read\")\n",
        "    if load_data:\n",
        "      self.claims, self.headlines, self.isQuestion,self.hasTowParts, self.labels =  self.__read_dataset(uniq_claims) \n",
        "    else:\n",
        "      self.claims, self.headlines, self.isQuestion,self.hasTowParts, self.labels = self.__generate_dataset(data)\n",
        "    print(\"after read\")\n",
        "    self.fa_punctuations = ['،','«','»',':','؛','ْ','ٌ','ٍ','ُ','ِ','َ','ّ','ٓ','ٰ','-','*']\n",
        "    self.denied_words = self.fa_stop_words + list(string.punctuation) + list(self.fa_punctuations)\n",
        "  # -------------------------------------\n",
        "  def __get_stop_words(self):   \n",
        "      normalizer = Normalizer()\n",
        "      lineList = list()\n",
        "      print(self.stopWord_path)\n",
        "      with open(self.stopWord_path) as f:\n",
        "        for line in f:\n",
        "          lineList.append(normalizer.normalize(line.rstrip(\"\\n\\r\")))\n",
        "      return lineList\n",
        "  # ---------------------------------------------------\n",
        "  def clean_sentence(self, sentence):\n",
        "    normalizer = Normalizer()\n",
        "    shayee = normalizer.normalize(\"شایعه\")\n",
        "    clean_sentences = sentence\n",
        "    re_pattern1 = \"(/(\\s)*\"+ shayee +\"(\\s)*[0-9]+)|(/(\\s)*شایعه(\\s)*[0-9]+)\"\n",
        "    re_pattern2 = \"/(\\s)*[0-9]+\"\n",
        "    re_pattern3 = \"\\\\u200c|\\\\u200d|\\\\u200e|\\\\u200b|\\\\u2067|\\\\u2069\"\n",
        "    x = re.search(re_pattern1, sentence)\n",
        "    if (x):\n",
        "      clean_sentences = re.sub(re_pattern1, \"\", sentence)\n",
        "\n",
        "    x = re.search(re_pattern2, clean_sentences)\n",
        "    if (x):\n",
        "      clean_sentences = re.sub(re_pattern2, \"\", clean_sentences)\n",
        "        \n",
        "    x = re.search(re_pattern3, clean_sentences)\n",
        "    if (x):\n",
        "      clean_sentences = re.sub(re_pattern3, \"\", clean_sentences)   \n",
        "        \n",
        "    punc_regex = re.compile('|'.join(map(re.escape, list(string.punctuation) + list(self.fa_punctuations))))\n",
        "\n",
        "    clean_sentences = punc_regex.sub(\"\", clean_sentences)\n",
        "\n",
        "    return clean_sentences\n",
        "  # ---------------------------------------------\n",
        "  def __generate_dataset(self,data):\n",
        "    claims = data[self.claim_name]\n",
        "    headline = data[self.headline_name]\n",
        "    b = np.char.find(claims, '?')\n",
        "    isQuestion = np.zeros(claims.shape[0])\n",
        "    isQuestion[np.where(b>0)] = 1\n",
        "    hasTowParts = np.zeros(claims.shape[0])\n",
        "    return claims, headlines,isQuestion,hasTowParts ,labels\n",
        "  # ---------------------------------------------\n",
        "  def __read_dataset(self,uniq_claims):\n",
        "    df = pd.read_csv(self.dataset_path, encoding = 'utf-8')\n",
        "    df = self.__remove_from_dataset(df,uniq_claims)\n",
        "    claims = df[self.claim_name].values\n",
        "    headlines = df[self.headline_name].values\n",
        "    print(\"before\")\n",
        "    isQuestion = df[self.question_name].values\n",
        "    print(\"after\")\n",
        "    hasTowParts = df[self.part_name].values\n",
        "    labels = df[self.label_name].values\n",
        "    print(isQuestion)\n",
        "    assert (claims.shape == headlines.shape == isQuestion.shape == labels.shape == hasTowParts.shape), \"The features size are not equal.\"\n",
        "    print(claims.shape , headlines.shape ,isQuestion.shape,hasTowParts.shape ,labels.shape)\n",
        "    return claims, headlines,isQuestion,hasTowParts ,labels\n",
        "  # ---------------------------------------\n",
        "  def __remove_from_dataset(self,df,uniq_claims):\n",
        "    uni_number = 0\n",
        "    if bool(uniq_claims):\n",
        "      df['repeated']= np.zeros(len(df),dtype=int)\n",
        "      for claim_index in range(len(df[self.claim_name])):\n",
        "        if (df[self.claim_name][claim_index] in uniq_claims) and uniq_claims[df[self.claim_name][claim_index]] == df[self.headline_name][claim_index]:\n",
        "          df['repeated'][claim_index] = 1\n",
        "          print(df[self.claim_name][claim_index])\n",
        "          print(df[self.headline_name][claim_index])\n",
        "          uni_number +=1\n",
        "      df.sort_values(by=['repeated'])\n",
        "      print(df['repeated'])\n",
        "    self.uniq_number = uni_number\n",
        "    return df\n",
        "  # ---------------------------------------\n",
        "\n",
        "  def stanford_tokenize(self, root_model_path, just_get_tokenized_words = False): \n",
        "    \n",
        "    nlp = stanfordnlp.Pipeline(lang='fa', models_dir= self.stanford_models_path, treebank=None, use_gpu=True) \n",
        "    # nlp = stanfordnlp.Pipeline(processors='tokenize,lemma', lang='fa', treebank=None, use_gpu=True)\n",
        "\n",
        "    claims_processors_result = []\n",
        "    headlines_processors_result = []\n",
        "    claims_tokenize = []\n",
        "    headlines_tokenize = []\n",
        "\n",
        "    for i, (claim,headline) in enumerate(zip(self.claims,self.headlines)):\n",
        "      clean_claim = self.clean_sentence(claim)\n",
        "      self.clean_claims.append(clean_claim)\n",
        "      doc = nlp(clean_claim) # Run the pipeline on input text\n",
        "      claims_processors_result.append(doc.sentences[0].words)\n",
        "      words = (obj.text for obj in doc.sentences[0].words)\n",
        "      claims_tokenize.append(words)\n",
        "\n",
        "      # headline\n",
        "      clean_headline = self.clean_sentence(headline)\n",
        "      self.clean_headlines.append(clean_headline)\n",
        "      doc = nlp(clean_headline) # Run the pipeline on input text\n",
        "      headlines_processors_result.append(doc.sentences[0].words)\n",
        "      words = (obj.text for obj in doc.sentences[0].words)\n",
        "      headlines_tokenize.append(words)\n",
        "      self.clean_claims_headlines.append(clean_claim + ' ' + clean_headline)\n",
        "\n",
        "    self.tokens_claims , self.tokens_headlines = self.clean_tokens(target_list = claims_tokenize), self.clean_tokens(target_list = headlines_tokenize)   \n",
        "    if just_get_tokenized_words :                \n",
        "      return self.tokens_claims , self.tokens_headlines \n",
        "# ghaGH\n",
        "    return claims_processors_result , headlines_processors_result\n",
        "  # ------------------------------------------------\n",
        "  \n",
        "  \n",
        "  def clean_tokens(self, target_list):\n",
        "    assert isinstance(target_list, (list)) == True , \"Type of target_list is not correct. It has to be list.\"\n",
        "    normalizer = Normalizer()\n",
        "    #--------!!!! It was [] and the following was commented, I uncomment it\n",
        "            \n",
        "    clean_words = []\n",
        "\n",
        "    for item in target_list:\n",
        "      clean_words.append([i for i in item if normalizer.normalize(i) not in self.denied_words])\n",
        "\n",
        "    return clean_words\n",
        "  # --------------------------------------------------\n",
        "  def hazm_tokenize(self):\n",
        "    claims_result = []\n",
        "    headlines_result = []\n",
        "    \n",
        "    for claim,headline in zip(self.claims,self.headlines):\n",
        "      clean_claim = self.clean_sentence(claim)\n",
        "      self.clean_claims.append(clean_claim)\n",
        "      claims_result.append(word_tokenize(clean_claim))\n",
        "      # headline\n",
        "      clean_headline = self.clean_sentence(headline)\n",
        "      self.clean_headlines.append(clean_headline)\n",
        "      headlines_result.append(word_tokenize(clean_headline))\n",
        "\n",
        "      self.clean_claims_headlines.append(clean_claim + ' ' + clean_headline)\n",
        "\n",
        "    self.tokens_claims , self.tokens_headlines = self.clean_tokens(target_list = claims_result), self.clean_tokens(target_list = headlines_result)\n",
        "    return self.tokens_claims , self.tokens_headlines\n",
        "\n",
        "  # --------------------------------------------------\n",
        "  def nltk_tokenize(self):\n",
        "    claims_result = []\n",
        "    headlines_result = []\n",
        "    \n",
        "    for claim,headline in zip(self.claims,self.headlines):\n",
        "      clean_claim = self.clean_sentence(claim)\n",
        "      self.clean_claims.append(clean_claim)\n",
        "      claims_result.append(nltk_word_tokenize(clean_claim))\n",
        "      \n",
        "      # headline\n",
        "      clean_headline = self.clean_sentence(headline)\n",
        "      self.clean_headlines.append(clean_headline)\n",
        "      headlines_result.append(nltk_word_tokenize(clean_headline))\n",
        "\n",
        "      self.clean_claims_headlines.append(clean_claim + ' ' + clean_headline)\n",
        "    self.tokens_claims , self.tokens_headlines = self.clean_tokens(target_list = claims_result), self.clean_tokens(target_list = headlines_result)\n",
        "    return self.tokens_claims , self.tokens_headlines\n",
        "  # --------------------------------------------------\n",
        "  def clean_sentences(self):\n",
        "    claims_result = []\n",
        "    headlines_result = []\n",
        "    for claim,headline in zip(self.claims,self.headlines):\n",
        "      tokens = parsbert_tokenizer.tokenize(claim)\n",
        "      clean_words = []\n",
        "      for token in tokens:\n",
        "          if token not in self.denied_words:\n",
        "              clean_words.append(token)\n",
        "      new_sentence_c = parsbert_tokenizer.convert_tokens_to_string(clean_words)\n",
        "      self.clean_claims.append(new_sentence_c)\n",
        "      \n",
        "      # headline\n",
        "      tokens = parsbert_tokenizer.tokenize(headline)\n",
        "      clean_words = []\n",
        "      for token in tokens:\n",
        "          if token not in self.denied_words:\n",
        "              clean_words.append(token)\n",
        "      new_sentence_h = parsbert_tokenizer.convert_tokens_to_string(clean_words)\n",
        "      self.clean_headlines.append(new_sentence_h)\n",
        "\n",
        "      self.clean_claims_headlines.append(new_sentence_c + ' ' + new_sentence_h)\n",
        "  # --------------------------------------------------\n",
        "  def tf_idf(self):\n",
        "    tfidf = TfidfVectorizer(sublinear_tf=True, min_df=10, norm='l2', ngram_range=(1, 2))\n",
        "    features = tfidf.fit_transform(self.clean_claims_headlines).toarray() \n",
        "    return features\n",
        "  # --------------------------------------------------\n",
        "  def tf_idf(self):\n",
        "    tfidf = TfidfVectorizer(sublinear_tf=True, min_df=10, norm='l2', ngram_range=(1, 2))\n",
        "    features = tfidf.fit_transform(self.clean_claims_headlines).toarray() \n",
        "    return features\n",
        "  # --------------------------------------------------\n",
        "  def similarity(self):\n",
        "    feature = []\n",
        "    for i, (claim,headline) in enumerate(zip(self.clean_claims,self.clean_headlines)):\n",
        "      ratio = SequenceMatcher(None, claim, headline).ratio()\n",
        "      quick_ratio = SequenceMatcher(None, claim, headline).quick_ratio()\n",
        "      real_quick_ratio = SequenceMatcher(None, claim, headline).real_quick_ratio()\n",
        "      feature.append([ratio,quick_ratio,real_quick_ratio])\n",
        "    return feature    \n",
        "  # --------------------------------------------------\n",
        "  def calc_important_words(self):\n",
        "    assert (self.important_words != None), 'For calculating important words you should pass important words in initializer.'\n",
        "    features = np.zeros((len(self.clean_claims_headlines), len(self.important_words)))\n",
        "    for i in range(len(self.clean_claims_headlines)):\n",
        "      for j in range(len(self.important_words)):\n",
        "        if self.important_words[j] in self.clean_claims_headlines[i]:\n",
        "            features[i][j] = 1\n",
        "    return features\n",
        "  # --------------------------------------------------\n",
        "  def calculate_root_distance(self ,target_sentences = None): # target_sentences = clean_headlines\n",
        "    \n",
        "    if target_sentences == None:\n",
        "      target_sentences = self.clean_headlines\n",
        "    \n",
        "    nlp = stanfordnlp.Pipeline(lang='fa', models_dir= self.stanford_models_path, treebank=None, use_gpu=True) \n",
        "    root_distance_feature = np.zeros((len(target_sentences),1))\n",
        "    for index,headline in enumerate(target_sentences):\n",
        "      root_distance_feature[index] = -1\n",
        "      doc = nlp(headline)\n",
        "      root = [(i,doc.sentences[0].words[i].text) for i in range(len(doc.sentences[0].words)) if  doc.sentences[0].words[i].dependency_relation == 'root' ]\n",
        "      if(len(root) == 0):\n",
        "        continue\n",
        "\n",
        "      root_index,root_word = root[0]\n",
        "\n",
        "      for word_index,word in enumerate(headline.split()) :\n",
        "        target = [(i,refute_hedge_reporte_words[i]) for i in range(len(refute_hedge_reporte_words)) if  refute_hedge_reporte_words[i] == word]\n",
        "        if(len(target) > 0):\n",
        "          target_index, target_word =target[0]\n",
        "          root_distance_feature[index] = abs(word_index - root_index)\n",
        "          break\n",
        "    return root_distance_feature\n",
        "  # --------------------------------------------------\n",
        "  def load_polarity_dataset(self):\n",
        "    excel = load_workbook(filename = self.polarity_dataset_path)\n",
        "    sheet = excel.active\n",
        "    words_polarity_fa={}\n",
        "    for row in sheet.iter_rows():\n",
        "      if row[2].value == \"Polarity\" or row[2].value == None:\n",
        "        continue\n",
        "      words_polarity_fa[row[0].value] = row[2].value\n",
        "    return words_polarity_fa  \n",
        "  # --------------------------------------------------\n",
        "  def calculate_polarity(self, target_sentences = None):\n",
        "    words_polarity_fa = self.load_polarity_dataset()\n",
        "\n",
        "    if target_sentences == None:\n",
        "      target_sentences = zip(self.tokens_claims,self.tokens_headlines)\n",
        "\n",
        "    # unzipping values \n",
        "    mapped = list(target_sentences)   \n",
        "    claims,headlines = zip(*mapped) \n",
        "    claims_array = np.asarray(claims)\n",
        "    polarity_vector = np.zeros((len(claims_array), 30))\n",
        "\n",
        "    for i,(claim,headline) in enumerate(zip(claims,headlines)):\n",
        "      j = 0\n",
        "      while j < len(claim) and j< 15:\n",
        "        if claim[j] in words_polarity_fa:\n",
        "          polarity_vector[i][j] = words_polarity_fa[claim[j]]\n",
        "        j += 1\n",
        "      j = 0\n",
        "      while j < len(headline) and j< 15:\n",
        "        if headline[j] in words_polarity_fa:\n",
        "          polarity_vector[i][j+15] = words_polarity_fa[headline[j]]          \n",
        "        j += 1\n",
        "    return polarity_vector\n",
        "  # --------------------------------------------------\n",
        "  # Function to average all word vectors in a paragraph\n",
        "  def __feature_vector_method(self, words, model, num_features):\n",
        "    # Pre-initialising empty numpy array for speed\n",
        "    featureVec = np.zeros(num_features,dtype=\"float32\")\n",
        "    nwords = 0\n",
        "    #Converting Index2Word which is a list to a set for better speed in the execution.\n",
        "    index2word_set = set(model.wv.index2word)\n",
        "    \n",
        "    for nwords, word in enumerate(words):\n",
        "      if word in index2word_set:\n",
        "        featureVec = np.add(featureVec,model[word])\n",
        "    \n",
        "    # Dividing the result by number of words to get average\n",
        "    featureVec = np.divide(featureVec, nwords)\n",
        "    return featureVec    \n",
        "  # --------------------------------------------------\n",
        "  def get_w2v_feature(self, model, num_features, target_sentences = None):\n",
        "    \n",
        "    if target_sentences == None:\n",
        "      target_sentences = self.clean_claims_headlines\n",
        "\n",
        "    reviewFeatureVecs = np.zeros((len(target_sentences),num_features),dtype=\"float32\")\n",
        "    for counter, sentence in enumerate(target_sentences):\n",
        "      # Printing a status message every 10000th review\n",
        "      if counter%1000 == 0:\n",
        "        print(\"data %d of %d\"%(counter,len(target_sentences)))\n",
        "          \n",
        "      reviewFeatureVecs[counter] = self.__feature_vector_method(sentence, model, num_features)\n",
        "        \n",
        "    return reviewFeatureVecs\n",
        "  # --------------------------------------------------\n",
        "  def get_bow(self, target_sentences = None):\n",
        "    if target_sentences == None:\n",
        "      target_sentences = self.clean_claims_headlines\n",
        "    vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
        "    X = vectorizer.fit_transform(target_sentences)\n",
        "    return X.toarray()\n",
        "  # --------------------------------------------------\n",
        "  def generate_Features(self, w2v_model_path, save_path, load_path, save_feature = False, load_if_exist = True, similarity = True, important_words = True, is_question = True, more_than2_parts = True, root_distance = True, polarity = True , w2v = True , bow = True ,tfidf = True):\n",
        "    features = self.isQuestion\n",
        "    features = np.reshape(features,(len(features),1))\n",
        "    file_name = ''\n",
        "\n",
        "    if load_if_exist == True or save_feature == True:\n",
        "      if tfidf:\n",
        "        file_name += 'tfidf_'\n",
        "      if similarity:\n",
        "        file_name += 'similarity_'\n",
        "      if important_words:\n",
        "        file_name += 'important_words_'        \n",
        "      if is_question:\n",
        "        file_name += 'is_question_'\n",
        "      if more_than2_parts:\n",
        "        file_name += 'more_than2_parts_'\n",
        "      if root_distance:\n",
        "        file_name += 'root_distance_'\n",
        "      if polarity:\n",
        "        file_name += 'polarity_'    \n",
        "      if w2v:\n",
        "        file_name += 'w2v_'     \n",
        "      if bow:\n",
        "        file_name += 'bow_'    \n",
        "\n",
        "\n",
        "    if load_if_exist :\n",
        "      assert len(load_path) > 0, \"Please enter load_path.\"\n",
        "      load_file_name = load_path + '/' + file_name + '.pkl'\n",
        "      if os.path.isfile(load_file_name) == True :\n",
        "        features = joblib.load(load_file_name)\n",
        "        print('Features loaded successfully.')\n",
        "        return features, file_name\n",
        "      else:\n",
        "        print('Features vector file is not exist.')      \n",
        "    # -------------- tfidf ----------\n",
        "    if tfidf:\n",
        "      print('Start to generate tf_idf feature')\n",
        "      tf_idf_feature = self.tf_idf()\n",
        "      features = np.append(features, tf_idf_feature ,axis = 1)\n",
        "      print('End of tf_idf feature')\n",
        "    # -------------- similarity ----------\n",
        "    if similarity:\n",
        "      print('Start to generate similarity feature')\n",
        "      similarity_feature = self.similarity()\n",
        "      features = np.append(features, similarity_feature ,axis = 1)\n",
        "      print('End of similarity feature')\n",
        "    # -------------- important words ----------\n",
        "    if important_words:\n",
        "      print('Start to generate important words feature')\n",
        "      important_words_feature = self.calc_important_words()\n",
        "      features = np.append(features, important_words_feature ,axis = 1)\n",
        "      print('End of important words feature')\n",
        "    # -------------- is question ----------\n",
        "    if is_question == False:\n",
        "      features = features[:,1:]\n",
        "    else:\n",
        "      print('\"is question\" feature was added.')\n",
        "    # -------------- more than tow parts ----------\n",
        "    if more_than2_parts:\n",
        "      features = np.append(features, np.reshape(self.hasTowParts, (len(self.hasTowParts),1)) ,axis = 1)\n",
        "      print('\"more than tow parts\" feature was added.')\n",
        "    # -------------- root distance ----------\n",
        "    if root_distance:\n",
        "      print('Start to generate root distance feature')\n",
        "      root_distance_feature = self.calculate_root_distance()\n",
        "      features = np.append(features, root_distance_feature ,axis = 1)\n",
        "      print('End of root distance feature')\n",
        "    # -------------- root distance ----------\n",
        "    if polarity:\n",
        "      print('Start to generate polarity feature')\n",
        "      polarity_feature = self.calculate_polarity()\n",
        "      features = np.append(features, polarity_feature ,axis = 1)\n",
        "      print('End of polarity feature')    \n",
        "    # -------------- w2v ----------\n",
        "    if w2v:\n",
        "      print('Start to generate w2v feature')\n",
        "      assert len(w2v_model_path) > 0, \"Please enter w2v_model_path.\"\n",
        "      w2v_model = joblib.load(w2v_model_path)\n",
        "      w2v_feature = self.get_w2v_feature(w2v_model, num_features = 300)\n",
        "      w2v_feature = (w2v_feature - np.min(w2v_feature))/ (np.max(w2v_feature) - np.min(w2v_feature))\n",
        "      features = np.append(features, w2v_feature ,axis = 1)\n",
        "      print('End of w2v feature')   \n",
        "    # -------------- bow ----------\n",
        "    if bow:\n",
        "      print('Start to generate bow feature')\n",
        "      bow_feature = self.get_bow()\n",
        "      features = np.append(features, bow_feature ,axis = 1)\n",
        "      print('End of bow feature')          \n",
        "    \n",
        "    if save_feature:\n",
        "      joblib.dump(features, (save_path + '/' + file_name + '.pkl'))\n",
        "      print('Features saved successfully.')          \n",
        "    return features, file_name\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YoJ_-nM-ZzZy",
        "outputId": "67bf77b8-385e-4936-9833-e8f8eff148a5"
      },
      "source": [
        "a = np.array(['dfmdk','dmiofvbioefbn?','dig?gsveiobvm'])\n",
        "b = np.char.find(a, '?')\n",
        "z = np.zeros(a.shape[0])\n",
        "z[np.where(b>0)] = 1\n",
        "z"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 1., 1.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uqrNtwUz4so"
      },
      "source": [
        "#Variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxF5KWtM_3-d"
      },
      "source": [
        "stanford_models_path  = '/content/drive/My Drive/persian_stance_baseline_data' \n",
        "stopWord_path = '/content/drive/My Drive/persian_stance_baseline_data/dataset/stopWords.txt'\n",
        "polarity_dataset_path = '/content/drive/My Drive/persian_stance_baseline_data/dataset/PerSent.xlsx'\n",
        "save_load_path = \"/content/drive/My Drive/persian_stance_baseline_data/vectors\"\n",
        "w2v_model_path = \"/content/drive/My Drive/persian_stance_baseline_data/vectors/w2v_persian.pkl\"\n",
        "train_test_sets_save_path = \"/content/drive/My Drive/persian_stance_baseline_data/dataset\"\n",
        "train_test_sets_load_path = \"/content/drive/My Drive/persian_stance_baseline_data/dataset\"\n",
        "train_path = \"/content/drive/My Drive/FullDS_train.csv\"\n",
        "test_path = \"/content/drive/My Drive/FullDS_test.csv\"\n",
        "full_path = '/content/drive/My Drive/FullDS.csv'# this full dataset\n",
        "model_path = \"HooshvareLab/bert-base-parsbert-uncased\""
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hS_LoGhkyMfa"
      },
      "source": [
        "MAX_LEN_a2c = 400  # max sequence length for the model\n",
        "MAX_LEN_h2c = 32  # max sequence length for the model\n",
        "GDRIVE_DIR = Path(\"/content/drive/My Drive\")  # path for google drive folder\n",
        "BEST_MODEL_PATH_a2c = '/content/drive/My Drive/multilingual_bert_dataset/a2c_parsbert_weights.h5'\n",
        "BEST_MODEL_PATH_h2c = '/content/drive/My Drive/h2c_parsbert_weights_WithExtraFeatures.h5'\n",
        "FA_PUNCTUATIONS = ['،','«','»',':','؛','ْ','ٌ','ٍ','ُ','ِ','َ','ّ','ٓ','ٰ','-','*']\n",
        "CLEAN_PUNCTUATION = False\n",
        "CLEAN_STOP_WORDS = False"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMxhC2BqF4Nu"
      },
      "source": [
        "#Bert"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7FfU300BTHF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0,
          "referenced_widgets": [
            "4d1f54d4e8ac4aa98a4436afab356362",
            "ed6b60230a9b4389b3f754d8a0228a9c",
            "a109cf24ae4b48efa3ebac7408c5a9a4",
            "52fac2971efb420d8af410a1d4628a60",
            "d606b3dbf212480b8da17bf6da0efbd0",
            "8f26f29a194d47d48d9020673eca3aa4",
            "1a988571cbb440b4a9434280945bce59",
            "fe002531c6ef4a8f9bc4754fb3742e40",
            "013053b03cb44a21a97c342d734e38e8",
            "47e138e2e8404964a328ff6dc241a747",
            "87a3b8eb5a51498e8576fc7038d5179f",
            "c94c32718be547bfba8a248ffc1ec798",
            "521af8d25735462cbe341271bc41b31c",
            "a523b8574ed44450bfc202635dcd92ce",
            "a40c644c422b428095bed5c2649f59b4",
            "d37c08f870824c7b9a7382356680a62c",
            "c0e22827cfc24d81ad6aec92088812a9",
            "af068e9b4ab146258a582f1acdd67d74",
            "acab944c30a34150867140b62e320215",
            "3ffab281780042038223b8c73f52d96c",
            "f8de5d0bda844d04b154e95e164e3082",
            "9bea7ef423414a36b22aa7d5aedc59d8",
            "abe4a796bb484d57a50dd7f5eb88f63a",
            "5419fbd1f4bf4947b5e92a21889b81ac"
          ]
        },
        "outputId": "bd3a669d-b50d-4bbd-9ba5-07cc97b517ed"
      },
      "source": [
        "parsbert_config = AutoConfig.from_pretrained(model_path, num_labels = 4)\n",
        "parsbert_tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "parsbert_model = TFAutoModel.from_pretrained(model_path, config = parsbert_config)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4d1f54d4e8ac4aa98a4436afab356362",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=434.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "013053b03cb44a21a97c342d734e38e8",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1215509.0, style=ProgressStyle(descript…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c0e22827cfc24d81ad6aec92088812a9",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=963211760.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at HooshvareLab/bert-base-parsbert-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at HooshvareLab/bert-base-parsbert-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "koWXw-VyBKgF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37d0c52b-ee20-4a58-a247-b4f51464ce93"
      },
      "source": [
        "labels = list(parsbert_config.label2id.keys())\n",
        "labels"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['LABEL_0', 'LABEL_1', 'LABEL_2', 'LABEL_3']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9K7_r30k1_B"
      },
      "source": [
        "###functions\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHE-3UoZ11vl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67c87f50-24b8-4831-f893-22d5cbb71906"
      },
      "source": [
        "\"\"\"\n",
        "Define our custom loss function.\n",
        "\"\"\"\n",
        "\n",
        "#FL1\n",
        "def categorical_focal_loss(gamma=2., alpha=.25):\n",
        "    \"\"\"\n",
        "    Softmax version of focal loss.\n",
        "           m\n",
        "      FL = ∑  -alpha * (1 - p_o,c)^gamma * y_o,c * log(p_o,c)\n",
        "          c=1\n",
        "      where m = number of classes, c = class and o = observation\n",
        "    Parameters:\n",
        "      alpha -- the same as weighing factor in balanced cross entropy\n",
        "      gamma -- focusing parameter for modulating factor (1-p)\n",
        "    Default value:\n",
        "      gamma -- 2.0 as mentioned in the paper\n",
        "      alpha -- 0.25 as mentioned in the paper\n",
        "    References:\n",
        "        Official paper: https://arxiv.org/pdf/1708.02002.pdf\n",
        "        https://www.tensorflow.org/api_docs/python/tf/keras/backend/categorical_crossentropy\n",
        "    Usage:\n",
        "     model.compile(loss=[categorical_focal_loss(alpha=.25, gamma=2)], metrics=[\"accuracy\"], optimizer=adam)\n",
        "    \"\"\"\n",
        "    def categorical_focal_loss_fixed(y_true, y_pred):\n",
        "        \"\"\"\n",
        "        :param y_true: A tensor of the same shape as `y_pred`\n",
        "        :param y_pred: A tensor resulting from a softmax\n",
        "        :return: Output tensor.\n",
        "        \"\"\"\n",
        "\n",
        "        # Scale predictions so that the class probas of each sample sum to 1\n",
        "        y_pred /= K.sum(y_pred, axis=-1, keepdims=True)\n",
        "\n",
        "        # Clip the prediction value to prevent NaN's and Inf's\n",
        "        epsilon = K.epsilon()\n",
        "        y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
        "\n",
        "        y_true = float(y_true)\n",
        "        # Calculate Cross Entropy\n",
        "        cross_entropy = -y_true * K.log(y_pred)\n",
        "\n",
        "        # Calculate Focal Loss\n",
        "        loss = alpha * K.pow(1 - y_pred, gamma) * cross_entropy\n",
        "\n",
        "        # Compute mean loss in mini_batch\n",
        "        return K.mean(loss, axis=1)\n",
        "\n",
        "    return categorical_focal_loss_fixed\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    # Test serialization of nested functions\n",
        "    cat_inner = dill.loads(dill.dumps(categorical_focal_loss(gamma=2., alpha=.25)))\n",
        "    print(cat_inner)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<function categorical_focal_loss_fixed at 0x7f19e043aa70>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSzdqzsD46HY"
      },
      "source": [
        "#Load Bert Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xs9YdHZ34PJL"
      },
      "source": [
        "##Article to claim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuzl9sAMw-fN"
      },
      "source": [
        "###Create"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lY38PqP4w96h"
      },
      "source": [
        "# BIAS INITILIZATION \n",
        "pi = 0.01\n",
        "b = -math.log((1-pi)/pi)\n",
        "bias_initializer = keras.initializers.Constant(value=b)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3jajK_cxCDG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7337f239-a7da-4d01-c82d-c3d71ad4dbf9"
      },
      "source": [
        "input_ids = layers.Input(shape=(MAX_LEN_a2c,), dtype=tf.int32, name = 'input_ids')\n",
        "token_type_ids = layers.Input(shape=(MAX_LEN_a2c,), dtype=tf.int32, name = 'token_type_ids')\n",
        "attention_mask = layers.Input(shape=(MAX_LEN_a2c,), dtype=tf.int32, name = 'attention_mask')\n",
        "\n",
        "bert = parsbert_model(\n",
        "    input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask\n",
        ")[0]\n",
        "\n",
        "hidden_mean = tf.reduce_mean(bert, axis=1)\n",
        "x = keras.layers.Dense(units = MAX_LEN_a2c, activation='relu')(hidden_mean)\n",
        "classifier = keras.layers.Dense(units = 4, name = 'classifier', activation = 'softmax')(x)\n",
        "# classifier = keras.layers.Dense(units = 4, name = 'classifier', activation = 'softmax', use_bias = True, \n",
        "#                                 bias_initializer = bias_initializer)(x)\n",
        "\n",
        "model_a2c = keras.Model(\n",
        "        inputs = [input_ids, token_type_ids, attention_mask],\n",
        "        outputs = [classifier],\n",
        "    )"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7f1aa1b68d70>> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7f1aa1b68d70>> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING:tensorflow:AutoGraph could not transform <function wrap at 0x7f1abcd1b170> and will run it as-is.\n",
            "Cause: while/else statement not yet supported\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <function wrap at 0x7f1abcd1b170> and will run it as-is.\n",
            "Cause: while/else statement not yet supported\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convertWARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py:5049: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9h4RoBIuxK2y"
      },
      "source": [
        "lr = 0.000035"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mHJ8oi_ucwpD"
      },
      "source": [
        "def get_f1(y_true, y_pred): #taken from old keras source code\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
        "    return f1_val"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0rAa7IcxNIe"
      },
      "source": [
        "loss = keras.losses.CategoricalCrossentropy()\n",
        "focal_loss = [categorical_focal_loss(alpha=0.25, gamma=2)]\n",
        "optimizer = keras.optimizers.Adam(learning_rate = lr)\n",
        "\n",
        "callbacks_list = [\n",
        "  EarlyStopping(monitor='val_loss', patience = 5),\n",
        "  ModelCheckpoint(filepath='/content/drive/My Drive/multilingual_bert_dataset/a2c_parsbert_weights.h5', save_best_only=True, save_weights_only = True, monitor='val_accuracy')\n",
        "]\n",
        "\n",
        "metrics_list = ['accuracy', keras.metrics.Precision(), keras.metrics.Recall(), get_f1]\n",
        "\n",
        "model_a2c.compile(optimizer = optimizer, loss = loss, metrics = metrics_list)\n",
        "# model.compile(optimizer = optimizer, loss = focal_loss, metrics = ['accuracy'])\n",
        "# model.summary()"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqFwu9-C4q_t"
      },
      "source": [
        "###Load"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBOKI7ScxRp9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "outputId": "6d7126c2-96b1-495a-c6cb-98d4ecdd9704"
      },
      "source": [
        "model_a2c.load_weights(BEST_MODEL_PATH_a2c)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-16e95df0a54e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_a2c\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBEST_MODEL_PATH_a2c\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch, options)\u001b[0m\n\u001b[1;32m   2317\u001b[0m           'first, then load the weights.')\n\u001b[1;32m   2318\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_assert_weights_created\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2319\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2320\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'layer_names'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrs\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m'model_weights'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2321\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_weights'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, **kwds)\u001b[0m\n\u001b[1;32m    425\u001b[0m                                fapl, fcpl=make_fcpl(track_order=track_order, fs_strategy=fs_strategy,\n\u001b[1;32m    426\u001b[0m                                fs_persist=fs_persist, fs_threshold=fs_threshold),\n\u001b[0;32m--> 427\u001b[0;31m                                swmr=swmr)\n\u001b[0m\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m             \u001b[0mflags\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r+'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: Unable to open file (unable to open file: name = '/content/drive/My Drive/multilingual_bert_dataset/a2c_parsbert_weights.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7Az-uTC33YE"
      },
      "source": [
        "##Headline to claim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwftJFLA6TwA"
      },
      "source": [
        "###create"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYS9AMy-6WFo"
      },
      "source": [
        "# BIAS INITILIZATION \n",
        "pi = 0.01\n",
        "b = -math.log((1-pi)/pi)\n",
        "bias_initializer = keras.initializers.Constant(value=b)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unihv5dO6Yi9"
      },
      "source": [
        "\n",
        "def create_model(batch_size =128, kernel_initializer= 'glorot_uniform', regularizer = None\n",
        "                 , dense_activity_regularizer=None, LSTM_implementation = 2, epochs = 8\n",
        "                 , dense_dropout = [0.5], lstm_dropout = [0.2], dense_neroun_count = [600]\n",
        "                 , lstm_neroun_count = [100], dense_activation = ['relu'] ,optimizer = 'adam'\n",
        "                 , other_feature_length= 10, print_summary= True, patience = 1, verbose= 1, save_path = '', alpha=0.25, gamma=2\n",
        "                 ,loss= 'kullback_leibler_divergence', learning_rate = 0.000035):\n",
        "  from keras.layers import Lambda \n",
        "\n",
        "  # Set Parameters\n",
        "  dense1_neroun_count = dense_neroun_count[0]  \n",
        "  dense2_neroun_count = dense_neroun_count[0]\n",
        "  dense3_neroun_count = dense_neroun_count[0]\n",
        "  \n",
        "  if(len(dense_neroun_count)>1):\n",
        "    dense2_neroun_count = dense_neroun_count[1]\n",
        "    \n",
        "  if(len(dense_neroun_count)>2):\n",
        "    dense3_neroun_count = dense_neroun_count[2]    \n",
        "  print('dense1_neroun_count:', dense1_neroun_count , 'dense2_neroun_count:',dense2_neroun_count,'dense3_neroun_count:',dense3_neroun_count)\n",
        "  #-----------------------\n",
        "  dense1_dropout = dense_dropout[0]  \n",
        "  dense2_dropout = dense_dropout[0]\n",
        "  dense3_dropout = dense_dropout[0]\n",
        "  \n",
        "  if(len(dense_dropout)>1):\n",
        "    dense2_dropout = dense_dropout[1]\n",
        "    \n",
        "  if(len(dense_dropout)>2):\n",
        "    dense3_dropout = dense_dropout[2]    \n",
        "  print('dense1_dropout:', dense1_dropout , 'dense2_dropout:',dense2_dropout,'dense3_dropout:',dense3_dropout)\n",
        "  #-----------------------  \n",
        "  dense1_activation = dense_activation[0]  \n",
        "  dense2_activation = dense_activation[0]\n",
        "  dense3_activation = dense_activation[0]\n",
        "  \n",
        "  if(len(dense_activation)>1):\n",
        "    dense2_activation = dense_activation[1]\n",
        "    \n",
        "  if(len(dense_activation)>2):\n",
        "    dense3_activation = dense_activation[2]    \n",
        "  print('dense1_activation:', dense1_activation , 'dense2_activation:',dense2_activation,'dense3_activation:',dense3_activation)  \n",
        "\n",
        "  # End of Set Parameters\n",
        "\n",
        "  input_ids = layers.Input(shape=(MAX_LEN_h2c,), dtype=tf.int32, name = 'input_ids')\n",
        "  token_type_ids = layers.Input(shape=(MAX_LEN_h2c,), dtype=tf.int32, name = 'token_type_ids')\n",
        "  attention_mask = layers.Input(shape=(MAX_LEN_h2c,), dtype=tf.int32, name = 'attention_mask')\n",
        "\n",
        "  bert = parsbert_model(\n",
        "      input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask\n",
        "  )[0]\n",
        "\n",
        "  bert_output = tf.reduce_mean(bert, axis=1)\n",
        "  # bert_output = keras.layers.Dense(units = dense1_neroun_count, activation=dense1_activation)(bert_output)\n",
        "  \n",
        "  mlp_input = keras.layers.Input(shape=(int(other_feature_length),), dtype='float32', name='mlp_input')\n",
        "\n",
        "  merged = keras.layers.concatenate([mlp_input, bert_output], axis = -1)\n",
        "\n",
        "  dense_mid = keras.layers.Dense(dense1_neroun_count, kernel_regularizer=regularizer, kernel_initializer=kernel_initializer,\n",
        "                            activity_regularizer=dense_activity_regularizer, activation= dense1_activation)(merged)\n",
        "\n",
        "  dense_mid = keras.layers.Dropout(dense1_dropout)(dense_mid)\n",
        "\n",
        "  dense_mid = keras.layers.Dense(dense2_neroun_count, kernel_regularizer=regularizer, kernel_initializer=kernel_initializer,\n",
        "                              activity_regularizer=dense_activity_regularizer, activation= dense2_activation)(dense_mid)\n",
        "\n",
        "  classifier = keras.layers.Dense(units = 4, name = 'classifier', activation = 'softmax', use_bias = True, \n",
        "                                  bias_initializer = bias_initializer)(dense_mid)\n",
        "\n",
        "  model = keras.Model(\n",
        "          inputs = [input_ids, token_type_ids, attention_mask, mlp_input],\n",
        "          outputs = [classifier],\n",
        "      )\n",
        "  \n",
        "  loss = keras.losses.CategoricalCrossentropy()\n",
        "  focal_loss = [categorical_focal_loss(alpha=alpha, gamma=gamma)]\n",
        "  optimizer = keras.optimizers.Adam(learning_rate)\n",
        "\n",
        "  metrics_list = ['accuracy', keras.metrics.Precision(), keras.metrics.Recall(), get_f1]\n",
        "  model.compile(optimizer = optimizer, loss = loss, metrics = metrics_list)\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8-yWHY16lQ2"
      },
      "source": [
        "save_path_best_model = \"\"\n",
        "save_path_best_model = '/content/drive/My Drive/h2c_parsbert_weights_WithExtraFeatures1.h5'\n",
        "model_h2c = create_model(batch_size =100, kernel_initializer= 'glorot_normal', regularizer = l2(0.015)\n",
        "                 , dense_activity_regularizer=regularizers.l1_l2(l1=1.1e-5, l2=1.01e-5), LSTM_implementation = 2, epochs = 5000\n",
        "                 , dense_dropout = [0.65]\n",
        "                 , dense_activation = ['relu'] ,optimizer = 'adam'\n",
        "                 , other_feature_length= 9, print_summary= False\n",
        "                 , save_path = save_path_best_model\n",
        "                 , patience = 5, dense_neroun_count = [200], alpha=0.5, gamma=1.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SDiYjQH7FIR"
      },
      "source": [
        "###load"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GE6fzKhY6spi"
      },
      "source": [
        "model_h2c.load_weights(BEST_MODEL_PATH_h2c)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDFQcyPiytKI"
      },
      "source": [
        "#Load Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bv7G7ZjXO8L"
      },
      "source": [
        "##Load_data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UREmPmH-DQ97"
      },
      "source": [
        "class Load_data():\n",
        "  def __init__(self):\n",
        "    self.load_data()\n",
        "    self.generate_features()\n",
        "\n",
        "\n",
        "  def convert_data_to_one_hot(self,y_train):\n",
        "    y_train_temp = np.zeros((y_train.shape[0], 4), dtype=np.int)\n",
        "    \n",
        "    for i,label in enumerate(y_train):\n",
        "      if(label == 'Agree'):\n",
        "        y_train_temp[i] = [0,0,0,1]\n",
        "      elif (label == 'Disagree'):\n",
        "        y_train_temp[i] = [0,0,1,0]\n",
        "      elif (label == 'Discuss'):\n",
        "        y_train_temp[i] = [0,1,0,0]\n",
        "      else :\n",
        "        y_train_temp[i] = [1,0,0,0]\n",
        "\n",
        "    return y_train_temp\n",
        "    \n",
        "  #load dataset and calculate yh2c and ya2c\n",
        "  def load_data(self):\n",
        "    self.psf_a2c = PSFeatureExtractor(dataset_path = full_path, stopWord_path = stopWord_path, polarity_dataset_path = polarity_dataset_path\n",
        "                                 , stanford_models_path =stanford_models_path , use_google_drive = True, important_words = ['؟','تکذیب','تکذیب شد',':',],claim_name=\"claim\"\n",
        "                                 ,headline_name=\"body\",label_name=\"body_stance\",question_name=\"question\" ,part_name=\"part\",uniq_claims={})\n",
        "    self.psf_h2c = PSFeatureExtractor(dataset_path = full_path, stopWord_path = stopWord_path, polarity_dataset_path = polarity_dataset_path\n",
        "                                    , stanford_models_path =stanford_models_path , use_google_drive = True, important_words = ['؟','تکذیب','تکذیب شد',':',],claim_name=\"claim\"\n",
        "                                    ,headline_name=\"headline\",label_name=\"headline_stance\",question_name=\"question\" ,part_name=\"part\",uniq_claims={})\n",
        "    labels_a2c = np.reshape(self.psf_a2c.labels,(len(self.psf_a2c.labels),1))\n",
        "    labels_h2c = np.reshape(self.psf_h2c.labels,(len(self.psf_h2c.labels),1))\n",
        "    self.y_a2c = self.convert_data_to_one_hot(labels_a2c)\n",
        "    self.y_h2c = self.convert_data_to_one_hot(labels_h2c)\n",
        "\n",
        "  #calculate concatinated x_a2c and x_h2c \n",
        "  def generate_features(self):\n",
        "    print(\"_______________\")\n",
        "    print(self.psf_a2c.claims.shape)\n",
        "    claims_array = self.psf_a2c.claims.reshape((self.psf_a2c.claims.shape[0], 1))\n",
        "    bodies_array = self.psf_a2c.headlines.reshape((self.psf_a2c.headlines.shape[0], 1))\n",
        "    headlines_array = self.psf_h2c.headlines.reshape((self.psf_a2c.headlines.shape[0], 1))\n",
        "    \n",
        "    self.psf_a2c.clean_sentences()\n",
        "    self.psf_h2c.clean_sentences()\n",
        "    features_a2c, features_name_a2c= self.psf_a2c.generate_Features(w2v_model_path = w2v_model_path,save_path = save_load_path, save_feature= False \n",
        "                                                               , load_path= save_load_path , root_distance = False , load_if_exist = False, tfidf = True, bow = False, w2v = False, polarity= False)\n",
        "    features_h2c, features_name_h2c= self.psf_h2c.generate_Features(w2v_model_path = w2v_model_path,save_path = save_load_path , save_feature= False\n",
        "                                                              , load_path= save_load_path , root_distance = False, load_if_exist = False, tfidf = False, bow = False, w2v = False , polarity= False)\n",
        "    \n",
        "    self.x_a2c = np.concatenate((claims_array, headlines_array, features_a2c), axis=1)\n",
        "    self.x_h2c = np.concatenate((claims_array, bodies_array, features_h2c), axis=1)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCEeJbhoXSXP"
      },
      "source": [
        "##Web"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AWNc6mqNO0C3"
      },
      "source": [
        "class Web():\n",
        "  def __init__(self):\n",
        "      self.total_num=1\n",
        "      self.correct_head=0\n",
        "      self.correct_body=0\n",
        "      self.false_head=0\n",
        "      self.false_body=0\n",
        "      self.cred_head=0\n",
        "      self.cred_body=0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCWdm8IRXUlW"
      },
      "source": [
        "##Credbility"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QEmfs3SRLI3i"
      },
      "source": [
        "class Credipility():\n",
        "  #data: stance of Load_data\n",
        "  #df : dataframe include web info\n",
        "  def __init__(self,data,model_h2c,model_a2c,df):\n",
        "    self.data = data\n",
        "    self.df = df\n",
        "    self.model_a2c = model_a2c\n",
        "    self.model_h2c = model_h2c\n",
        "    self.cal_cred_dic()\n",
        "\n",
        "  def cal_body(self,i,stance_h,stance_b,domain):\n",
        "    if stance_b == 'Agree':\n",
        "      if self.df['veracity'][i] == 'True':\n",
        "        self.web_dic[domain].correct_body +=1\n",
        "        self.web_dic[domain].cred_body +=1\n",
        "        return 0,1\n",
        "      elif  self.df['veracity'][i] == 'False':\n",
        "        self.web_dic[domain].cred_body -=1\n",
        "        self.web_dic[domain].false_body +=1\n",
        "        return 1,0\n",
        "    elif stance_b == 'Disagree':\n",
        "      if self.df['veracity'][i] == 'True':\n",
        "        self.web_dic[domain].cred_body -=1\n",
        "        self.web_dic[domain].false_body +=1\n",
        "        return 1,0\n",
        "      elif  self.df['veracity'][i] == 'False':\n",
        "        self.web_dic[domain].correct_body +=1 \n",
        "        self.web_dic[domain].cred_body +=1\n",
        "        return 0,1\n",
        "    elif stance_b == 'Discuss':\n",
        "      a = self.create_inputs_targets(np.array([self.data.x_a2c[i]]),MAX_LEN=MAX_LEN_a2c)\n",
        "      stance_a2c = self.model_a2c.predict([a[0],a[1], a[2]])\n",
        "      truth_a2c= stance_a2c[0][3] -stance_a2c[0][2] /stance_a2c[0][3]+stance_a2c[0][2]\n",
        "      if self.df['veracity'][i] == 'True':\n",
        "        self.web_dic[domain].cred_body += truth_a2c\n",
        "      elif  self.df['veracity'][i] == 'False':\n",
        "        self.web_dic[domain].cred_body -= truth_a2c\n",
        "    return 0,0\n",
        "\n",
        "  def cal_header(self,i,stance_h,stance_b,domain):\n",
        "    if stance_h == 'Agree':\n",
        "      if self.df['veracity'][i] == 'True':\n",
        "        self.web_dic[domain].correct_head +=1\n",
        "        self.web_dic[domain].cred_head +=1\n",
        "        return 0,1\n",
        "      elif  self.df['veracity'][i] == 'False':\n",
        "        self.web_dic[domain].cred_head -=1\n",
        "        self.web_dic[domain].false_head +=1\n",
        "        return 1,0\n",
        "    elif stance_h == 'Disagree':\n",
        "      if self.df['veracity'][i] == 'True':\n",
        "        self.web_dic[domain].cred_head -=1\n",
        "        self.web_dic[domain].false_head +=1\n",
        "        return 1,0\n",
        "      elif  self.df['veracity'][i] == 'False':\n",
        "        self.web_dic[domain].correct_head +=1 \n",
        "        self.web_dic[domain].cred_head +=1\n",
        "        return 0,1\n",
        "    elif stance_h == 'Discuss':\n",
        "      a = self.create_inputs_targets(np.array([self.data.x_h2c[i]]),MAX_LEN=MAX_LEN_h2c)\n",
        "      stance_h2c = self.model_h2c.predict([a[0],a[1], a[2],tf.convert_to_tensor([self.data.x_h2c[0,2:]])])\n",
        "      truth_h2c= stance_h2c[0][3] - stance_h2c[0][2] /stance_h2c[0][3]+stance_h2c[0][2]\n",
        "      if self.df['veracity'][i] == 'True':\n",
        "        self.web_dic[domain].cred_head += truth_h2c\n",
        "      elif  self.df['veracity'][i] == 'False':\n",
        "        self.web_dic[domain].cred_head -= truth_h2c\n",
        "    return 0,0\n",
        "\n",
        "  def cal_credibility(self):\n",
        "    self.web_dic = {}\n",
        "    total_num_truth = 0\n",
        "    total_num_falseness = 0\n",
        "    for i in range(len(df)):\n",
        "      stance_h = self.df['headline_stance'][i]\n",
        "      stance_b = self.df['body_stance'][i]\n",
        "      domain = self.df['web_domain'][i]\n",
        "      if domain not in self.web_dic:\n",
        "        self.web_dic[domain]=Web()\n",
        "      else: \n",
        "        self.web_dic[domain].total_num +=1\n",
        "      fh,th = self.cal_header(i,stance_h,stance_b,domain)\n",
        "      fb,tb = self.cal_body(i,stance_h,stance_b,domain)\n",
        "      total_num_truth += tb\n",
        "      total_num_falseness +=fb  \n",
        "\n",
        "  def create_inputs_targets(self,x_raw, padding = True,MAX_LEN = MAX_LEN_a2c):\n",
        "    dataset_dict = {\n",
        "          \"input_ids\": [],\n",
        "          \"token_type_ids\": [],\n",
        "          \"attention_mask\": []\n",
        "          }\n",
        "\n",
        "    LENGTH = len(x_raw)\n",
        "    for i in range(LENGTH):\n",
        "        input = x_raw[i] # an array has two strings inside: headline, calim\n",
        "\n",
        "        body = input[0]\n",
        "        claim = input[1]\n",
        "\n",
        "        sequence = parsbert_tokenizer(body, claim, padding = 'max_length', max_length = MAX_LEN, truncation=True)\n",
        "        input_ids = sequence['input_ids']  # encoded tokens\n",
        "        token_type_ids = sequence['token_type_ids']  # segment number for every token. 0 for headline. 1 for claim.\n",
        "        attention_mask = sequence['attention_mask']\n",
        "\n",
        "        dataset_dict[\"input_ids\"].append(input_ids)\n",
        "        dataset_dict[\"token_type_ids\"].append(token_type_ids)\n",
        "        dataset_dict[\"attention_mask\"].append(attention_mask)\n",
        "\n",
        "    for key in dataset_dict:\n",
        "        dataset_dict[key] = tf.convert_to_tensor(dataset_dict[key])\n",
        "\n",
        "    x = [\n",
        "          dataset_dict[\"input_ids\"],\n",
        "          dataset_dict[\"token_type_ids\"],\n",
        "          dataset_dict[\"attention_mask\"],\n",
        "        ]\n",
        "    x = tf.convert_to_tensor(x)\n",
        "    return x\n",
        "    \n",
        "  def cal_cred_dic(self): \n",
        "    self.x_a2c = self.create_inputs_targets(self.data.x_a2c,MAX_LEN=MAX_LEN_a2c)\n",
        "    self.x_h2c = self.create_inputs_targets(self.data.x_h2c,MAX_LEN=MAX_LEN_h2c)\n",
        "    self.tf_MLP_input = tf.convert_to_tensor(self.data.x_h2c[:, 2:], dtype=tf.float32)\n",
        "    self.cal_credibility()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YlJXHKzRXYkF"
      },
      "source": [
        "##Extract_features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tY9StCcGIrUL"
      },
      "source": [
        "class Extract_features():\n",
        "  def __init__(self,df,data):\n",
        "    self.data = data\n",
        "    self.df = df\n",
        "    self.df_len = len(self.df)\n",
        "    self.set_webinfo()\n",
        "    self.set_importantwords()\n",
        "    self.cred = None\n",
        "    \n",
        "  def set_webinfo(self):\n",
        "    self.df['web_host'] = np.empty(self.df_len,dtype=np.str)\n",
        "    self.df['web_domain'] = np.empty(self.df_len,dtype=np.str)\n",
        "    self.df['transfer_protocol'] = np.ones(self.df_len)*0.5\n",
        "    for i in range(len(df)):\n",
        "      try:\n",
        "        domain = urlparse(self.df['url'][i]).netloc\n",
        "        domain = domain.split('.')\n",
        "        self.df['web_host'][i] = domain[-1]\n",
        "        self.df['web_domain'][i] = domain[-2]\n",
        "        if self.df['url'][i][4] == 's':\n",
        "          self.df['transfer_protocol'][i] = 1\n",
        "        elif not df['url'][i][4] == ':':\n",
        "          self.df['transfer_protocol'][i] = 0    \n",
        "      except: \n",
        "        self.df['web_host'][i] = '-'\n",
        "        self.df['web_domain'][i] = '-'\n",
        "        self.df['transfer_protocol'][i] = 0 \n",
        "  \n",
        "  def set_importantwords(self):\n",
        "    self.data.psf_a2c.important_words = refute_hedge_reporte_words\n",
        "    self.data.psf_h2c.important_words = refute_hedge_reporte_words\n",
        "    featrues_important_words_a2c = self.data.psf_a2c.calc_important_words()\n",
        "    featrues_important_words_h2c = self.data.psf_h2c.calc_important_words()\n",
        "    self.df =pd.concat([self.df, pd.DataFrame(featrues_important_words_h2c+featrues_important_words_a2c)], axis=1)\n",
        "\n",
        "  def set_web_cred(self):\n",
        "    # self.df['credibility_head'] = np.ones(self.df_len)*0.1\n",
        "    # self.df['credibility_body'] = np.ones(self.df_len)*0.1\n",
        "    # self.df['tot_correct_head'] = np.zeros(self.df_len)\n",
        "    # self.df['tot_correct_body'] = np.zeros(self.df_len)\n",
        "    # self.df['tot_false_head'] = np.zeros(self.df_len)\n",
        "    # self.df['tot_false_body'] = np.zeros(self.df_len)\n",
        "    # self.df['tot_article'] = np.zeros(self.df_len)\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yj8CRxAeXbVK"
      },
      "source": [
        "##Fakenews"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5gGEkJAAVFg"
      },
      "source": [
        "class Fakenews():\n",
        "  def __init__(self,model,df):\n",
        "    self.model = model\n",
        "    self.df = df\n",
        "    self.data_loader = Load_data()\n",
        "    self.data_loader.load_data()\n",
        "    self.data_loader.generate_features()\n",
        "    feature_extractor = Extract_features(df,fn.data_loader)\n",
        "    feature_extractor.cred = Credipility(data,model_h2c,model_a2c,self.df)\n",
        "    feature_extractor.set_web_cred()\n",
        "\n",
        "  def Train(self):\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3cRktDrgxKa"
      },
      "source": [
        "fn = Fakenews(None,df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpE5AHheehdQ"
      },
      "source": [
        "print(feature_extractor.cred.web_dic.keys())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKS12hHGzPoj"
      },
      "source": [
        "#Calculate stance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZGKqdepoSgTY"
      },
      "source": [
        "def save_ds(ds_col,csv_path):\n",
        "  print('converting text data to csv file..')\n",
        "  claim = ds_col[0]\n",
        "  headline = ds_col[1]\n",
        "  body = ds_col[2]\n",
        "  url = ds_col[3]\n",
        "  headline_stance = ds_col[4]\n",
        "  body_stance = ds_col[5]\n",
        "  veracity = ds_col[6]\n",
        "  question = ds_col[7]\n",
        "  part = ds_col[8]\n",
        "  index = np.array(range(ds_col.shape[1]))\n",
        "\n",
        "      \n",
        "  Dataset = list(zip(index, claim, headline, body, question, part, veracity, url,headline_stance,body_stance))\n",
        "  print(Dataset[0])\n",
        "  np.random.shuffle(Dataset)\n",
        "      \n",
        "  df = pd.DataFrame(data = Dataset, columns=['index', 'claim', 'headline', 'body', 'question', 'part', 'veracity', 'url', 'headline_stance', 'body_stance'])\n",
        "  df.to_csv(csv_path, index=False, encoding=\"utf-8\")\n",
        "\n",
        "  print('done!') \n",
        "  return Dataset,df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XksHJA4SSnDH"
      },
      "source": [
        "import glob\n",
        "\n",
        "# txt to CSV\n",
        "def load_save_ds(data_path,csv_path):\n",
        "  data_file = glob.glob(data_path)\n",
        "  row_documents = []\n",
        "  cnt = 1\n",
        "\n",
        "  for file in data_file:\n",
        "      with open (file, \"r\", encoding=\"utf-8\") as fp:\n",
        "          line = fp.readline() #first line is for the headers (Claim, Body Text, Claim Is Question, Claim Has Tow Parts, Stance)\n",
        "          content = fp.read() #read the rest of the file to a string\n",
        "  content = content.replace('\\n',' ')\n",
        "  row_documents = content.split(\"_eoc_\") #split the instances by the custom delimiter\n",
        "  row_documents = list(filter(None, row_documents)) #remove empty instance\n",
        "  row_documents = [row.strip() for row in  row_documents]\n",
        "\n",
        "  #remove last empty element\n",
        "  row_documents = row_documents[:-1]\n",
        "  #conver text into 2d array (9*num_sample)\n",
        "  doc_num = len(row_documents)\n",
        "  num_sample = int(doc_num/9)\n",
        "  print(\"number of samples : \", num_sample)\n",
        "  ds_col = np.array(row_documents).reshape(num_sample,9).T\n",
        "  return save_ds(ds_col,csv_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVYorl7oSrZY"
      },
      "source": [
        "def split_test_train(Dataset,ds_col,train_path,test_path):\n",
        "  claims = []\n",
        "  headlines = []\n",
        "  bodys = []\n",
        "  uniqs = []\n",
        "  train=[]\n",
        "  count = 0\n",
        "  for i in range(len(Dataset)):\n",
        "    if Dataset[i][1] in claims and count <= 0.1*len(ds):\n",
        "      headlines.append(Dataset[i][2])\n",
        "      bodys.append(Dataset[i][3])\n",
        "      uniqs.append(Dataset[i])\n",
        "      count +=1\n",
        "    else:\n",
        "        claims.append(Dataset[i][1])\n",
        "        train.append(Dataset[i])\n",
        "  #save_train\n",
        "  train = pd.DataFrame(data = train, columns=['index', 'claim', 'headline', 'body', 'question', 'part', 'veracity', 'url', 'headline_stance', 'body_stance'])\n",
        "  train.to_csv(train_path, index=False, encoding=\"utf-8\")\n",
        "  \n",
        "  #save_test\n",
        "  test = pd.DataFrame(data = uniqs, columns=['index', 'claim', 'headline', 'body', 'question', 'part', 'veracity', 'url', 'headline_stance', 'body_stance'])\n",
        "  test.to_csv(test_path, index=False, encoding=\"utf-8\")\n",
        "\n",
        "  print('done!')\n",
        "  return claims, headlines, bodys"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOAt8rDhS3bN"
      },
      "source": [
        "# data_path = '/content/drive/My Drive/Copy of FullDataset_990627.txt'\n",
        "# full_path = '/content/drive/My Drive/FullDS.csv'\n",
        "# dataset, df =  load_save_ds(data_path,full_path)\n",
        "df = pd.read_csv(train_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVI0m0nJtzlQ"
      },
      "source": [
        "df = pd.read_csv(train_path, encoding = 'utf-8')\n",
        "claims = df[self.claim_name].values\n",
        "headlines = df[self.headline_name].values\n",
        "print(\"before\")\n",
        "isQuestion = df[self.question_name].values\n",
        "print(\"after\")\n",
        "hasTowParts = df[self.part_name].values\n",
        "labels = df[self.label_name].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8liD_mH9Ypf"
      },
      "source": [
        "test_claims, test_headlines, test_bodys = split_test_train(ds,data_path,train_path,test_path)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}